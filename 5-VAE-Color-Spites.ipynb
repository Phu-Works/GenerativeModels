{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/deepmind/dsprites-dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: \n",
      " {b'date': b'April 2017', b'description': b'Disentanglement test Sprites dataset.Procedurally generated 2D shapes, from 6 disentangled latent factors.This dataset uses 6 latents, controlling the color, shape, scale, rotation and position of a sprite. All possible variations of the latents are present. Ordering along dimension 1 is fixed and can be mapped back to the exact latent values that generated that image.We made sure that the pixel outputs are different. No noise added.', b'version': 1, b'latents_names': (b'color', b'shape', b'scale', b'orientation', b'posX', b'posY'), b'latents_possible_values': {b'orientation': array([0.        , 0.16110732, 0.32221463, 0.48332195, 0.64442926,\n",
      "       0.80553658, 0.96664389, 1.12775121, 1.28885852, 1.44996584,\n",
      "       1.61107316, 1.77218047, 1.93328779, 2.0943951 , 2.25550242,\n",
      "       2.41660973, 2.57771705, 2.73882436, 2.89993168, 3.061039  ,\n",
      "       3.22214631, 3.38325363, 3.54436094, 3.70546826, 3.86657557,\n",
      "       4.02768289, 4.1887902 , 4.34989752, 4.51100484, 4.67211215,\n",
      "       4.83321947, 4.99432678, 5.1554341 , 5.31654141, 5.47764873,\n",
      "       5.63875604, 5.79986336, 5.96097068, 6.12207799, 6.28318531]), b'posX': array([0.        , 0.03225806, 0.06451613, 0.09677419, 0.12903226,\n",
      "       0.16129032, 0.19354839, 0.22580645, 0.25806452, 0.29032258,\n",
      "       0.32258065, 0.35483871, 0.38709677, 0.41935484, 0.4516129 ,\n",
      "       0.48387097, 0.51612903, 0.5483871 , 0.58064516, 0.61290323,\n",
      "       0.64516129, 0.67741935, 0.70967742, 0.74193548, 0.77419355,\n",
      "       0.80645161, 0.83870968, 0.87096774, 0.90322581, 0.93548387,\n",
      "       0.96774194, 1.        ]), b'posY': array([0.        , 0.03225806, 0.06451613, 0.09677419, 0.12903226,\n",
      "       0.16129032, 0.19354839, 0.22580645, 0.25806452, 0.29032258,\n",
      "       0.32258065, 0.35483871, 0.38709677, 0.41935484, 0.4516129 ,\n",
      "       0.48387097, 0.51612903, 0.5483871 , 0.58064516, 0.61290323,\n",
      "       0.64516129, 0.67741935, 0.70967742, 0.74193548, 0.77419355,\n",
      "       0.80645161, 0.83870968, 0.87096774, 0.90322581, 0.93548387,\n",
      "       0.96774194, 1.        ]), b'scale': array([0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), b'shape': array([1., 2., 3.]), b'color': array([1.])}, b'latents_sizes': array([ 1,  3,  6, 40, 32, 32]), b'author': b'lmatthey@google.com', b'title': b'dSprites dataset'}\n"
     ]
    }
   ],
   "source": [
    "## Code from Deepmind's Github\n",
    "load_data = np.load(\"dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\", encoding='bytes')\n",
    "imgs = load_data['imgs']\n",
    "latents_values = load_data['latents_values']\n",
    "latents_classes = load_data['latents_classes']\n",
    "metadata = load_data['metadata'][()]\n",
    "\n",
    "experiment_name = \"VAESpriteColor\"\n",
    "\n",
    "print('Metadata: \\n', metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_sizes = metadata[b'latents_sizes']\n",
    "latents_bases = np.concatenate((latents_sizes[::-1].cumprod()[::-1][1:],\n",
    "                                np.array([1,])))\n",
    "\n",
    "def latent_to_index(latents):\n",
    "    return np.dot(latents, latents_bases).astype(int)\n",
    "\n",
    "def sample_latent(size=1):\n",
    "    samples = np.zeros((size, latents_sizes.size))\n",
    "    for lat_i, lat_size in enumerate(latents_sizes):\n",
    "        samples[:, lat_i] = np.random.randint(lat_size, size=size)\n",
    "\n",
    "    return samples\n",
    "\n",
    "def show_images_grid(imgs_, num_images=25, cmap='Greys_r'):\n",
    "    ncols = int(np.ceil(num_images**0.5))\n",
    "    nrows = int(np.ceil(num_images / ncols))\n",
    "    _, axes = plt.subplots(ncols, nrows, figsize=(nrows * 3, ncols * 3))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax_i, ax in enumerate(axes):\n",
    "        if ax_i < num_images:\n",
    "            if not cmap is None:\n",
    "                ax.imshow(imgs_[ax_i], cmap=cmap,  interpolation='nearest')\n",
    "            else:\n",
    "                ax.imshow(imgs_[ax_i]*255, interpolation='nearest')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        else:\n",
    "            ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182af81c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latents_sampled = sample_latent(size=60000)\n",
    "latents_sampled[:, 1] = 1\n",
    "indices_sampled = latent_to_index(latents_sampled)\n",
    "imgs_sampled = imgs[indices_sampled]\n",
    "\n",
    "show_images_grid(imgs_sampled, 9, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_sampled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_color(base_image):\n",
    "    all_color = itertools.product([0, 1], repeat=3)\n",
    "    all_images = []\n",
    "    for r, g, b in all_color:\n",
    "        if (r, g, b) == (0, 0, 0):\n",
    "            continue\n",
    "        new_color = np.stack([\n",
    "            base_image*r,\n",
    "            base_image*g,\n",
    "            base_image*b\n",
    "        ], axis=-1)\n",
    "\n",
    "        all_images.append(new_color)\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now at 0/60000\n",
      "Now at 10000/60000\n",
      "Now at 20000/60000\n",
      "Now at 30000/60000\n",
      "Now at 40000/60000\n",
      "Now at 50000/60000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(420000, 64, 64, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images_color = []\n",
    "\n",
    "for i, img in enumerate(imgs_sampled):\n",
    "    if i%10000 == 0:\n",
    "        print(f\"Now at {i}/{imgs_sampled.shape[0]}\")\n",
    "    all_images_color += create_color(img)\n",
    "    \n",
    "imgs_sampled_color = np.stack(all_images_color)\n",
    "imgs_sampled_color.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18e37d5588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images_grid(imgs_sampled_color, 9, cmap=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BUF = 60000\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(imgs_sampled_color).shuffle(TRAIN_BUF).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAESprite(tf.keras.Model):\n",
    "    \"\"\"Same Architecture\"\"\"\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(64, 64, 3)),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=4, strides=(2, 2), activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=4, strides=(2, 2), activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=4, strides=(2, 2), activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=4, strides=(2, 2), activation=tf.nn.elu),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "        ])\n",
    "        \n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "            tf.keras.layers.Dense(units=2*2*64, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Reshape(target_shape=(2, 2, 64)),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=4, strides=(2, 2), padding=\"SAME\", activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=4, strides=(2, 2), padding=\"SAME\", activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=4, strides=(4, 4), padding=\"SAME\", activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=2, strides=(2, 2), padding=\"SAME\", activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=1, strides=(1, 1), padding=\"SAME\"),\n",
    "        ])\n",
    "        \n",
    "    def sample(self):\n",
    "        latent = tf.random_normal(shape=(1, self.latent_dim))\n",
    "        return latent, tf.nn.sigmoid(self.decoder(latent))\n",
    "    \n",
    "    def call(self, img, is_sigmoid=False):\n",
    "        \"\"\"Reuse the code from the Google Example\"\"\"\n",
    "        mean, log_var = tf.split(self.encoder(img), num_or_size_splits=2, axis=1)\n",
    "        \n",
    "        normal = tf.random_normal(shape=mean.shape)\n",
    "        latent = normal * tf.exp(log_var * .5) + mean\n",
    "        \n",
    "        out = self.decoder(latent)\n",
    "        if is_sigmoid:\n",
    "            out = tf.nn.sigmoid(out)\n",
    "        return latent, out, mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAESprite(10)\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "summary_writer = tf.contrib.summary.create_file_writer(f\"tmp/{experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(5e-5)\n",
    "saver = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                            model=vae,\n",
    "                            optimizer_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "save_path = f\"save/{experiment_name}\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0\n",
      "tf.Tensor(8517.852, shape=(), dtype=float32)\n",
      "tf.Tensor(6333.1523, shape=(), dtype=float32)\n",
      "tf.Tensor(2570.6301, shape=(), dtype=float32)\n",
      "tf.Tensor(1944.098, shape=(), dtype=float32)\n",
      "tf.Tensor(1639.5856, shape=(), dtype=float32)\n",
      "tf.Tensor(1592.9497, shape=(), dtype=float32)\n",
      "tf.Tensor(1322.1505, shape=(), dtype=float32)\n",
      "tf.Tensor(1251.6619, shape=(), dtype=float32)\n",
      "tf.Tensor(1137.6254, shape=(), dtype=float32)\n",
      "tf.Tensor(918.9296, shape=(), dtype=float32)\n",
      "tf.Tensor(816.0204, shape=(), dtype=float32)\n",
      "tf.Tensor(740.4628, shape=(), dtype=float32)\n",
      "tf.Tensor(771.4777, shape=(), dtype=float32)\n",
      "tf.Tensor(714.4378, shape=(), dtype=float32)\n",
      "tf.Tensor(678.9746, shape=(), dtype=float32)\n",
      "tf.Tensor(640.14014, shape=(), dtype=float32)\n",
      "tf.Tensor(585.5236, shape=(), dtype=float32)\n",
      "tf.Tensor(634.49023, shape=(), dtype=float32)\n",
      "tf.Tensor(622.6235, shape=(), dtype=float32)\n",
      "tf.Tensor(572.5793, shape=(), dtype=float32)\n",
      "tf.Tensor(564.6291, shape=(), dtype=float32)\n",
      "tf.Tensor(535.23834, shape=(), dtype=float32)\n",
      "tf.Tensor(593.2396, shape=(), dtype=float32)\n",
      "tf.Tensor(558.0656, shape=(), dtype=float32)\n",
      "tf.Tensor(542.4804, shape=(), dtype=float32)\n",
      "tf.Tensor(535.83307, shape=(), dtype=float32)\n",
      "tf.Tensor(491.06046, shape=(), dtype=float32)\n",
      "tf.Tensor(521.03357, shape=(), dtype=float32)\n",
      "tf.Tensor(477.18518, shape=(), dtype=float32)\n",
      "tf.Tensor(478.14047, shape=(), dtype=float32)\n",
      "tf.Tensor(458.34393, shape=(), dtype=float32)\n",
      "tf.Tensor(494.3836, shape=(), dtype=float32)\n",
      "tf.Tensor(497.6102, shape=(), dtype=float32)\n",
      "tf.Tensor(504.89453, shape=(), dtype=float32)\n",
      "tf.Tensor(480.38086, shape=(), dtype=float32)\n",
      "tf.Tensor(505.39197, shape=(), dtype=float32)\n",
      "tf.Tensor(464.8705, shape=(), dtype=float32)\n",
      "tf.Tensor(502.73715, shape=(), dtype=float32)\n",
      "tf.Tensor(472.78122, shape=(), dtype=float32)\n",
      "tf.Tensor(445.9868, shape=(), dtype=float32)\n",
      "tf.Tensor(504.72107, shape=(), dtype=float32)\n",
      "tf.Tensor(464.25543, shape=(), dtype=float32)\n",
      "---------\n",
      "At epoch 1\n",
      "tf.Tensor(463.46817, shape=(), dtype=float32)\n",
      "tf.Tensor(488.96222, shape=(), dtype=float32)\n",
      "tf.Tensor(430.94766, shape=(), dtype=float32)\n",
      "tf.Tensor(471.98642, shape=(), dtype=float32)\n",
      "tf.Tensor(441.26385, shape=(), dtype=float32)\n",
      "tf.Tensor(484.47852, shape=(), dtype=float32)\n",
      "tf.Tensor(458.87296, shape=(), dtype=float32)\n",
      "tf.Tensor(460.54794, shape=(), dtype=float32)\n",
      "tf.Tensor(449.0572, shape=(), dtype=float32)\n",
      "tf.Tensor(458.90915, shape=(), dtype=float32)\n",
      "tf.Tensor(442.22238, shape=(), dtype=float32)\n",
      "tf.Tensor(434.256, shape=(), dtype=float32)\n",
      "tf.Tensor(442.17288, shape=(), dtype=float32)\n",
      "tf.Tensor(446.1803, shape=(), dtype=float32)\n",
      "tf.Tensor(452.23254, shape=(), dtype=float32)\n",
      "tf.Tensor(462.70538, shape=(), dtype=float32)\n",
      "tf.Tensor(456.7892, shape=(), dtype=float32)\n",
      "tf.Tensor(419.74167, shape=(), dtype=float32)\n",
      "tf.Tensor(430.05807, shape=(), dtype=float32)\n",
      "tf.Tensor(437.26157, shape=(), dtype=float32)\n",
      "tf.Tensor(425.1816, shape=(), dtype=float32)\n",
      "tf.Tensor(420.28937, shape=(), dtype=float32)\n",
      "tf.Tensor(433.90173, shape=(), dtype=float32)\n",
      "tf.Tensor(419.36365, shape=(), dtype=float32)\n",
      "tf.Tensor(447.5716, shape=(), dtype=float32)\n",
      "tf.Tensor(452.98474, shape=(), dtype=float32)\n",
      "tf.Tensor(422.36273, shape=(), dtype=float32)\n",
      "tf.Tensor(455.92734, shape=(), dtype=float32)\n",
      "tf.Tensor(412.89032, shape=(), dtype=float32)\n",
      "tf.Tensor(449.67172, shape=(), dtype=float32)\n",
      "tf.Tensor(412.89297, shape=(), dtype=float32)\n",
      "tf.Tensor(432.79895, shape=(), dtype=float32)\n",
      "tf.Tensor(442.80875, shape=(), dtype=float32)\n",
      "tf.Tensor(426.95197, shape=(), dtype=float32)\n",
      "tf.Tensor(443.0621, shape=(), dtype=float32)\n",
      "tf.Tensor(424.6036, shape=(), dtype=float32)\n",
      "tf.Tensor(396.17712, shape=(), dtype=float32)\n",
      "tf.Tensor(431.26, shape=(), dtype=float32)\n",
      "tf.Tensor(428.9033, shape=(), dtype=float32)\n",
      "tf.Tensor(421.1496, shape=(), dtype=float32)\n",
      "tf.Tensor(419.98575, shape=(), dtype=float32)\n",
      "tf.Tensor(398.69016, shape=(), dtype=float32)\n",
      "---------\n",
      "At epoch 2\n",
      "tf.Tensor(379.12823, shape=(), dtype=float32)\n",
      "tf.Tensor(394.80203, shape=(), dtype=float32)\n",
      "tf.Tensor(403.03387, shape=(), dtype=float32)\n",
      "tf.Tensor(433.24246, shape=(), dtype=float32)\n",
      "tf.Tensor(385.05792, shape=(), dtype=float32)\n",
      "tf.Tensor(434.2007, shape=(), dtype=float32)\n",
      "tf.Tensor(397.44894, shape=(), dtype=float32)\n",
      "tf.Tensor(378.53372, shape=(), dtype=float32)\n",
      "tf.Tensor(393.38406, shape=(), dtype=float32)\n",
      "tf.Tensor(447.15762, shape=(), dtype=float32)\n",
      "tf.Tensor(400.17358, shape=(), dtype=float32)\n",
      "tf.Tensor(387.05823, shape=(), dtype=float32)\n",
      "tf.Tensor(409.75348, shape=(), dtype=float32)\n",
      "tf.Tensor(385.32788, shape=(), dtype=float32)\n",
      "tf.Tensor(378.74786, shape=(), dtype=float32)\n",
      "tf.Tensor(344.6804, shape=(), dtype=float32)\n",
      "tf.Tensor(312.67273, shape=(), dtype=float32)\n",
      "tf.Tensor(346.34943, shape=(), dtype=float32)\n",
      "tf.Tensor(310.02472, shape=(), dtype=float32)\n",
      "tf.Tensor(298.72934, shape=(), dtype=float32)\n",
      "tf.Tensor(309.77844, shape=(), dtype=float32)\n",
      "tf.Tensor(276.6728, shape=(), dtype=float32)\n",
      "tf.Tensor(295.75873, shape=(), dtype=float32)\n",
      "tf.Tensor(286.51505, shape=(), dtype=float32)\n",
      "tf.Tensor(268.54498, shape=(), dtype=float32)\n",
      "tf.Tensor(287.37408, shape=(), dtype=float32)\n",
      "tf.Tensor(282.80426, shape=(), dtype=float32)\n",
      "tf.Tensor(269.45688, shape=(), dtype=float32)\n",
      "tf.Tensor(277.24368, shape=(), dtype=float32)\n",
      "tf.Tensor(277.10095, shape=(), dtype=float32)\n",
      "tf.Tensor(275.1659, shape=(), dtype=float32)\n",
      "tf.Tensor(269.93787, shape=(), dtype=float32)\n",
      "tf.Tensor(262.89276, shape=(), dtype=float32)\n",
      "tf.Tensor(254.01732, shape=(), dtype=float32)\n",
      "tf.Tensor(261.67444, shape=(), dtype=float32)\n",
      "tf.Tensor(247.51653, shape=(), dtype=float32)\n",
      "tf.Tensor(243.76306, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for e in range(5):\n",
    "    print(f\"At epoch {e}\")\n",
    "    for i, img in enumerate(train_dataset):\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(vae.variables)\n",
    "\n",
    "            latent, out, mean, log_var = vae(img)\n",
    "            \n",
    "            # Reconstruction loss ? \n",
    "            cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=out, labels=img)\n",
    "            logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "            \n",
    "            # KL Loss -- From https://wiseodd.github.io/techblog/2017/01/24/vae-pytorch/\n",
    "            kl_loss = 0.5 * tf.reduce_sum(tf.exp(log_var) + mean**2 - 1. - log_var, axis=[1])\n",
    "            \n",
    "            loss = -tf.reduce_mean(logpx_z - kl_loss)\n",
    "            if i % 100 == 0:\n",
    "                with summary_writer.as_default(), tf.contrib.summary.always_record_summaries():\n",
    "                    tf.contrib.summary.scalar('Loss', loss)\n",
    "                    tf.contrib.summary.image('Before', img)\n",
    "                    tf.contrib.summary.image('Sample', out)\n",
    "                    \n",
    "                print(loss)\n",
    "                \n",
    "        global_step.assign_add(1)\n",
    "        grad = tape.gradient(loss, vae.variables)\n",
    "        optimizer.apply_gradients(zip(grad, vae.variables))\n",
    "        \n",
    "    print(\"---------\")   \n",
    "    saver.save(f\"save/{experiment_name}/{experiment_name}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tf.nn.sigmoid(out[4]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_latent, img = vae.sample()\n",
    "img = tf.squeeze(img)\n",
    "\n",
    "plt.imshow(img.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_sampled = sample_latent(size=1)\n",
    "\n",
    "latents_sampled[:, 1] = 0\n",
    "indices_sampled = latent_to_index(latents_sampled)\n",
    "imgs_sampled = imgs[indices_sampled]\n",
    "\n",
    "test_img = imgs_sampled[0]\n",
    "test_color_img = create_color(test_img)\n",
    "plt.imshow(test_color_img[3]*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_color_img_single = tf.expand_dims(tf.convert_to_tensor(test_color_img[3]), axis=0)\n",
    "latent, out, _, _ = vae(tf.cast(test_color_img_single, tf.float32))\n",
    "\n",
    "out = tf.squeeze(tf.nn.sigmoid(out))\n",
    "plt.imshow(out.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = latent - sampled_latent\n",
    "number_steps = 10\n",
    "diff = diff/number_steps \n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle = sampled_latent\n",
    "frames = []\n",
    "for i in range(number_steps):\n",
    "    out = tf.nn.sigmoid(vae.decoder(middle))\n",
    "    out = tf.squeeze(out)\n",
    "    frames.append(out.numpy())\n",
    "    middle += diff\n",
    "\n",
    "frames = np.stack(frames)\n",
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_grid(frames, num_images=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_latent(latent, axis, traverse_length=9, total_step=9):\n",
    "    step_size = traverse_length * 2 / total_step\n",
    "    item = latent[:, axis]\n",
    "    \n",
    "    item_evolution = []\n",
    "    item = item - traverse_length\n",
    "    import copy\n",
    "    for i in range(total_step):\n",
    "        new_latent = copy.deepcopy(latent)\n",
    "        new_latent[:, axis] = item\n",
    "        item_evolution.append(new_latent)\n",
    "        item += step_size\n",
    "        \n",
    "    traversed_latent = np.squeeze(np.stack(item_evolution))\n",
    "    return traversed_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_latent = traverse_latent(latent.numpy(), 0, total_step=10)\n",
    "out = vae.decoder(tf.convert_to_tensor(all_latent))\n",
    "out = tf.squeeze(tf.nn.sigmoid(out))\n",
    "\n",
    "show_images_grid(out.numpy(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
