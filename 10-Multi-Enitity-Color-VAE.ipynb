{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "experiment_name = \"VAEMultipleShapesWithColor\"\n",
    "is_load_save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dsprites_multiple_shapes4.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(975100, 64, 64, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Data \n",
    "data_root = 'dsprites-dataset/ColorMultipleData/upload'\n",
    "\n",
    "for file in os.listdir(data_root):\n",
    "    if '.npz' in file:\n",
    "        print(f\"Loading {file}\")\n",
    "        dataset = np.load(os.path.join(data_root, file))['data']\n",
    "        break\n",
    "\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[:100000].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_grid(imgs_, num_images=25, is_grey=False):\n",
    "    ncols = int(np.ceil(num_images**0.5))\n",
    "    nrows = int(np.ceil(num_images / ncols))\n",
    "    _, axes = plt.subplots(ncols, nrows, figsize=(nrows * 3, ncols * 3))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax_i, ax in enumerate(axes):\n",
    "        if ax_i < num_images:\n",
    "            if is_grey:\n",
    "                ax.imshow(imgs_[ax_i], cmap='gray', interpolation='nearest')\n",
    "            else:\n",
    "                ax.imshow(imgs_[ax_i], interpolation='nearest')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        else:\n",
    "            ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182d912ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images_grid(dataset[100000-9:], num_images=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BUF = 100000\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(dataset).shuffle(TRAIN_BUF).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAESprite(tf.keras.Model):\n",
    "    \"\"\"Same Architecture\"\"\"\n",
    "    def __init__(self, latent_dim, num_object):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_object = num_object\n",
    "        \n",
    "        self.encoder1 = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(64, 64, 3)),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=4, strides=(2, 2), activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=1, strides=(1, 1), activation=tf.nn.elu),\n",
    "        ])\n",
    "        \n",
    "        self.encoder2 = tf.keras.Sequential([       \n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=4, strides=(2, 2), activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2D(filters=24, kernel_size=1, strides=(1, 1), activation=tf.nn.elu),\n",
    "        ])\n",
    "        \n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "            tf.keras.layers.Dense(units=4*4*64, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Reshape(target_shape=(4, 4, 64)),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=4, strides=(2, 2), padding=\"SAME\", activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=4, strides=(2, 2), padding=\"SAME\", activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=2, strides=(2, 2), padding=\"SAME\", activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=2, strides=(2, 2), padding=\"SAME\", activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=1, strides=(1, 1), padding=\"SAME\"),\n",
    "        ])\n",
    "    \n",
    "    def call(self, img, is_sigmoid=False):\n",
    "        \"\"\"Reuse the code from the Google Example\"\"\"\n",
    "        encoder1 = self.encoder1(img)\n",
    "        \n",
    "        # Adding x, y coordinate\n",
    "        x = tf.convert_to_tensor([-15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, \n",
    "                                  0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])/15\n",
    "        y = tf.convert_to_tensor([-15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, \n",
    "                                  0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])/15\n",
    "        \n",
    "        X, Y = tf.meshgrid(x, y)\n",
    "        X, Y = tf.expand_dims(tf.cast(X, tf.float32), -1), tf.expand_dims(tf.cast(Y, tf.float32), -1)\n",
    "        X, Y = tf.expand_dims(tf.cast(X, tf.float32), 0), tf.expand_dims(tf.cast(Y, tf.float32), 0)\n",
    "        \n",
    "        X, Y = tf.tile(X, [encoder1.shape[0], 1, 1, 1]), tf.tile(X, [encoder1.shape[0], 1, 1, 1])\n",
    "        \n",
    "        encoder_pos = tf.concat([encoder1, X, Y], -1)\n",
    "        encoder_final = self.encoder2(encoder_pos)\n",
    "\n",
    "        encoder_flatten = tf.reshape(encoder_final, (tf.shape(encoder_final)[0], \n",
    "                                                    -1, tf.shape(encoder_final)[-1]))\n",
    "        mean, log_var = tf.split(encoder_flatten, num_or_size_splits=2, axis=-1)\n",
    "        \n",
    "        # Getting Analytic-KL\n",
    "        kl = 0.5 * tf.reduce_sum(tf.exp(log_var) + mean**2 - 1. - log_var, axis=[-1])\n",
    "        total_kl_value, total_kl_index = tf.math.top_k(kl, k=self.num_object)\n",
    "        \n",
    "        # Getting top 2\n",
    "        top_latent_mean = tf.batch_gather(mean, total_kl_index)\n",
    "        top_latent_log_var = tf.batch_gather(log_var, total_kl_index)\n",
    "        \n",
    "        # Getting latents for each objects \n",
    "        normal = tf.random_normal(shape=top_latent_mean.shape)\n",
    "        each_latent = normal * tf.exp(top_latent_log_var * .5) + top_latent_mean\n",
    "        \n",
    "        # Should I do this ? \n",
    "        latents = tf.reshape(each_latent, (-1, self.latent_dim))\n",
    "        decoded = self.decoder(latents)\n",
    "        \n",
    "        imgs = tf.reshape(decoded, (-1, self.num_object, decoded.shape[1], \n",
    "                                       decoded.shape[2], decoded.shape[3]))\n",
    "        \n",
    "        # Add all images \n",
    "        final_images = tf.reduce_sum(imgs, axis=1)\n",
    "        return each_latent, final_images, top_latent_mean, top_latent_log_var, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 12\n",
    "object_number = 2\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "def get_learning_rate():\n",
    "    global learning_rate\n",
    "    learning_rate *= 0.9999984649444495\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAESprite(latent_size, object_number)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=get_learning_rate)\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "summary_writer = tf.contrib.summary.create_file_writer(f\"tmp/{experiment_name}\")\n",
    "\n",
    "saver = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                            model=vae,\n",
    "                            optimizer_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "save_path = f\"save/{experiment_name}\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0\n",
      "tf.Tensor([100  14  14  24], shape=(4,), dtype=int32)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c28789777e24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mflat_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-e33b760005de>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, img, is_sigmoid)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mencoder_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         encoder_flatten = tf.reshape(encoder_final, (tf.shape(encoder_final)[0], \n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(40):\n",
    "    print(f\"At epoch {e}\")\n",
    "    for i, img in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(vae.variables)\n",
    "\n",
    "            latent, out, mean, log_var, kl_map = vae(img)\n",
    "            \n",
    "            flat_mean = tf.reshape(mean, (-1, latent_size))\n",
    "            flat_log_var = tf.reshape(log_var, (-1, latent_size))\n",
    "\n",
    "            cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=out, labels=img)\n",
    "            logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "            kl_loss = 0.5 * tf.reduce_sum(tf.exp(flat_log_var) + flat_mean**2 - 1. - flat_log_var, \n",
    "                                          axis=[1])\n",
    "            kl_loss = tf.reduce_sum(tf.reshape(kl_loss, (-1, object_number)), axis=1)\n",
    "            loss = -tf.reduce_mean(logpx_z - kl_loss)\n",
    "            \n",
    "            print(loss)\n",
    "            \n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Loss -- {loss}   Current Learning Rate {learning_rate}\")\n",
    "                with summary_writer.as_default(), tf.contrib.summary.always_record_summaries():\n",
    "                    tf.contrib.summary.scalar('Loss', loss)\n",
    "                    tf.contrib.summary.image('Before', img)\n",
    "                    tf.contrib.summary.image('Sample', tf.nn.sigmoid(out))\n",
    "                    \n",
    "        \n",
    "        grad = tape.gradient(loss, vae.variables)\n",
    "        global_step.assign_add(1)\n",
    "        optimizer.apply_gradients(zip(grad, vae.variables))\n",
    "        assert False\n",
    "        \n",
    "    print(f\"---------\")\n",
    "    saver.save(f\"save/{experiment_name}/{experiment_name}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_grid(tf.squeeze(img).numpy(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_grid(tf.squeeze(tf.nn.sigmoid(out)).numpy(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl_attention_map(kl_index):\n",
    "    batch_size = kl_index.shape[0]\n",
    "    blank_map = np.zeros((batch_size, 14*14))\n",
    "    for i in range(batch_size):\n",
    "        blank_map[i] = kl_index[i]\n",
    "    return blank_map.reshape(batch_size, 14, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_grid(get_kl_attention_map(kl_map), 3, is_grey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
