{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "experiment_name = \"VAEMultipleShapesNoColor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('dsprites-dataset/dsprites_multiple_shapes_no_color.npz.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_grid(imgs_, num_images=25):\n",
    "    ncols = int(np.ceil(num_images**0.5))\n",
    "    nrows = int(np.ceil(num_images / ncols))\n",
    "    _, axes = plt.subplots(ncols, nrows, figsize=(nrows * 3, ncols * 3))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax_i, ax in enumerate(axes):\n",
    "        if ax_i < num_images:\n",
    "            ax.imshow(imgs_[ax_i], cmap='Greys_r',  interpolation='nearest')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        else:\n",
    "            ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182bc18668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images_grid(np.squeeze(dataset), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[:100000].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 64, 64, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BUF = 100000\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(dataset).shuffle(TRAIN_BUF).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAESprite(tf.keras.Model):\n",
    "    \"\"\"Same Architecture\"\"\"\n",
    "    def __init__(self, latent_dim, num_object):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_object = num_object\n",
    "        \n",
    "        self.encoder1 = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(64, 64, 1)),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=4, strides=(2, 2), activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2D(filters=32, kernel_size=2, strides=(1, 1), activation=tf.nn.elu),\n",
    "        ])\n",
    "        \n",
    "        self.encoder2 = tf.keras.Sequential([       \n",
    "            tf.keras.layers.Conv2D(filters=64, kernel_size=4, strides=(2, 2), activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2D(filters=24, kernel_size=1, strides=(1, 1), activation=tf.nn.elu),\n",
    "        ])\n",
    "        \n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "            tf.keras.layers.Dense(units=4*4*64, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Reshape(target_shape=(4, 4, 64)),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=4, strides=(2, 2), padding=\"SAME\", activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=4, strides=(2, 2), padding=\"SAME\", activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=2, strides=(2, 2), padding=\"SAME\", activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=2, strides=(2, 2), padding=\"SAME\", activation=tf.nn.elu),\n",
    "            tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=1, strides=(1, 1), padding=\"SAME\"),\n",
    "        ])\n",
    "    \n",
    "    def call(self, img, is_sigmoid=False):\n",
    "        \"\"\"Reuse the code from the Google Example\"\"\"\n",
    "        encoder1 = self.encoder1(img)\n",
    "        \n",
    "        # Adding x, y coordinate\n",
    "        x = tf.convert_to_tensor([-14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, \n",
    "                                  0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])/15\n",
    "        y = tf.convert_to_tensor([-14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, \n",
    "                                  0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])/15\n",
    "        \n",
    "        X, Y = tf.meshgrid(x, y)\n",
    "        X, Y = tf.expand_dims(tf.cast(X, tf.float32), -1), tf.expand_dims(tf.cast(Y, tf.float32), -1)\n",
    "        X, Y = tf.expand_dims(tf.cast(X, tf.float32), 0), tf.expand_dims(tf.cast(Y, tf.float32), 0)\n",
    "        \n",
    "        X, Y = tf.tile(X, [encoder1.shape[0], 1, 1, 1]), tf.tile(X, [encoder1.shape[0], 1, 1, 1])\n",
    "        \n",
    "        encoder_pos = tf.concat([encoder1, X, Y], -1)\n",
    "        encoder_final = self.encoder2(encoder_pos)\n",
    "        \n",
    "        encoder_flatten = tf.reshape(encoder_final, (tf.shape(encoder_final)[0], \n",
    "                                                    -1, tf.shape(encoder_final)[-1]))\n",
    "        mean, log_var = tf.split(encoder_flatten, num_or_size_splits=2, axis=-1)\n",
    "        \n",
    "        # Getting Analytic-KL\n",
    "        kl = 0.5 * tf.reduce_sum(tf.exp(log_var) + mean**2 - 1. - log_var, axis=[-1])\n",
    "        total_kl_value, total_kl_index = tf.math.top_k(kl, k=self.num_object)\n",
    "        \n",
    "        # Getting top 2\n",
    "        top_latent_mean = tf.batch_gather(mean, total_kl_index)\n",
    "        top_latent_log_var = tf.batch_gather(log_var, total_kl_index)\n",
    "        \n",
    "        # Getting latents for each objects \n",
    "        normal = tf.random_normal(shape=top_latent_mean.shape)\n",
    "        each_latent = normal * tf.exp(top_latent_log_var * .5) + top_latent_mean\n",
    "        \n",
    "        # Should I do this ? \n",
    "        latents = tf.reshape(each_latent, (-1, self.latent_dim))\n",
    "        decoded = self.decoder(latents)\n",
    "        \n",
    "        imgs = tf.reshape(decoded, (-1, self.num_object, decoded.shape[1], \n",
    "                                       decoded.shape[2], decoded.shape[3]))\n",
    "        \n",
    "        # Add all images \n",
    "        final_images = tf.reduce_sum(imgs, axis=1)\n",
    "        return each_latent, final_images, top_latent_mean, top_latent_log_var, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 12\n",
    "object_number = 2\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "def get_learning_rate():\n",
    "    global learning_rate\n",
    "    learning_rate *= 0.9999984649444495\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAESprite(latent_size, object_number)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=get_learning_rate)\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "summary_writer = tf.contrib.summary.create_file_writer(f\"tmp/{experiment_name}\")\n",
    "\n",
    "saver = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                            model=vae,\n",
    "                            optimizer_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "save_path = f\"save/{experiment_name}\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0\n",
      "Loss -- 2839.8603515625   Current Learning Rate 0.001\n",
      "Loss -- 846.439453125   Current Learning Rate 0.0009998465061085268\n",
      "Loss -- 782.0558471679688   Current Learning Rate 0.00099969303577743\n",
      "Loss -- 756.008544921875   Current Learning Rate 0.0009995395890030907\n",
      "Loss -- 756.0664672851562   Current Learning Rate 0.0009993861657818946\n",
      "Loss -- 725.5579833984375   Current Learning Rate 0.000999232766110225\n",
      "Loss -- 685.2756958007812   Current Learning Rate 0.0009990793899844694\n",
      "Loss -- 724.1875610351562   Current Learning Rate 0.0009989260374010107\n",
      "Loss -- 713.190185546875   Current Learning Rate 0.0009987727083562378\n",
      "Loss -- 710.5396118164062   Current Learning Rate 0.0009986194028465364\n",
      "---------\n",
      "At epoch 1\n",
      "Loss -- 687.5068969726562   Current Learning Rate 0.000998466120868294\n",
      "Loss -- 665.0042114257812   Current Learning Rate 0.0009983128624178985\n",
      "Loss -- 676.4312744140625   Current Learning Rate 0.0009981596274917398\n",
      "Loss -- 632.8388061523438   Current Learning Rate 0.000998006416086206\n",
      "Loss -- 600.59423828125   Current Learning Rate 0.0009978532281976875\n",
      "Loss -- 647.9116821289062   Current Learning Rate 0.0009977000638225733\n",
      "Loss -- 625.1541748046875   Current Learning Rate 0.000997546922957255\n",
      "Loss -- 595.69677734375   Current Learning Rate 0.000997393805598125\n",
      "Loss -- 610.2819213867188   Current Learning Rate 0.000997240711741573\n",
      "Loss -- 583.1603393554688   Current Learning Rate 0.0009970876413839941\n",
      "---------\n",
      "At epoch 2\n",
      "Loss -- 635.9364624023438   Current Learning Rate 0.0009969345945217802\n",
      "Loss -- 563.1931762695312   Current Learning Rate 0.0009967815711513237\n",
      "Loss -- 586.0106201171875   Current Learning Rate 0.000996628571269021\n",
      "Loss -- 540.5   Current Learning Rate 0.0009964755948712637\n",
      "Loss -- 550.287841796875   Current Learning Rate 0.0009963226419544508\n",
      "Loss -- 554.3297729492188   Current Learning Rate 0.000996169712514975\n",
      "Loss -- 561.5505981445312   Current Learning Rate 0.0009960168065492351\n",
      "Loss -- 553.718017578125   Current Learning Rate 0.000995863924053625\n",
      "Loss -- 540.473876953125   Current Learning Rate 0.0009957110650245446\n",
      "Loss -- 535.0001220703125   Current Learning Rate 0.0009955582294583927\n",
      "---------\n",
      "At epoch 3\n",
      "Loss -- 476.1275634765625   Current Learning Rate 0.000995405417351565\n",
      "Loss -- 538.709228515625   Current Learning Rate 0.0009952526287004626\n",
      "Loss -- 500.9958190917969   Current Learning Rate 0.000995099863501486\n",
      "Loss -- 475.8747253417969   Current Learning Rate 0.0009949471217510339\n",
      "Loss -- 509.8214111328125   Current Learning Rate 0.0009947944034455077\n",
      "Loss -- 481.7130126953125   Current Learning Rate 0.0009946417085813087\n",
      "Loss -- 523.6798706054688   Current Learning Rate 0.0009944890371548377\n",
      "Loss -- 482.1979675292969   Current Learning Rate 0.000994336389162499\n",
      "Loss -- 497.8852233886719   Current Learning Rate 0.0009941837646006942\n",
      "Loss -- 472.6963806152344   Current Learning Rate 0.0009940311634658273\n",
      "---------\n",
      "At epoch 4\n",
      "Loss -- 492.45428466796875   Current Learning Rate 0.0009938785857543028\n",
      "Loss -- 459.9945983886719   Current Learning Rate 0.0009937260314625256\n",
      "Loss -- 441.86663818359375   Current Learning Rate 0.0009935735005869003\n",
      "Loss -- 479.6502380371094   Current Learning Rate 0.00099342099312383\n",
      "Loss -- 440.9586181640625   Current Learning Rate 0.0009932685090697257\n",
      "Loss -- 464.02117919921875   Current Learning Rate 0.0009931160484209919\n",
      "Loss -- 418.5863952636719   Current Learning Rate 0.0009929636111740358\n",
      "Loss -- 464.7412109375   Current Learning Rate 0.0009928111973252667\n",
      "Loss -- 459.2226867675781   Current Learning Rate 0.0009926588068710916\n",
      "Loss -- 426.98370361328125   Current Learning Rate 0.0009925064398079208\n",
      "---------\n",
      "At epoch 5\n",
      "Loss -- 427.243896484375   Current Learning Rate 0.0009923540961321632\n",
      "Loss -- 419.617919921875   Current Learning Rate 0.0009922017758402302\n",
      "Loss -- 465.1674499511719   Current Learning Rate 0.0009920494789285312\n",
      "Loss -- 457.56402587890625   Current Learning Rate 0.0009918972053934774\n",
      "Loss -- 452.9472961425781   Current Learning Rate 0.0009917449552314809\n",
      "Loss -- 374.5153503417969   Current Learning Rate 0.0009915927284389538\n",
      "Loss -- 384.0203857421875   Current Learning Rate 0.00099144052501231\n",
      "Loss -- 398.5840759277344   Current Learning Rate 0.0009912883449479624\n",
      "Loss -- 406.9785461425781   Current Learning Rate 0.0009911361882423257\n",
      "Loss -- 396.375244140625   Current Learning Rate 0.000990984054891814\n",
      "---------\n",
      "At epoch 6\n",
      "Loss -- 432.1849670410156   Current Learning Rate 0.000990831944892842\n",
      "Loss -- 397.4654235839844   Current Learning Rate 0.0009906798582418259\n",
      "Loss -- 392.9268798828125   Current Learning Rate 0.000990527794935181\n",
      "Loss -- 398.1177062988281   Current Learning Rate 0.0009903757549693255\n",
      "Loss -- 395.8213195800781   Current Learning Rate 0.0009902237383406767\n",
      "Loss -- 403.90753173828125   Current Learning Rate 0.0009900717450456494\n",
      "Loss -- 398.5286865234375   Current Learning Rate 0.0009899197750806667\n",
      "Loss -- 364.0135498046875   Current Learning Rate 0.0009897678284421453\n",
      "Loss -- 392.2386779785156   Current Learning Rate 0.0009896159051265036\n",
      "Loss -- 357.4605712890625   Current Learning Rate 0.0009894640051301632\n",
      "---------\n",
      "At epoch 7\n",
      "Loss -- 391.769775390625   Current Learning Rate 0.0009893121284495444\n",
      "Loss -- 400.88800048828125   Current Learning Rate 0.0009891602750810677\n",
      "Loss -- 355.8218688964844   Current Learning Rate 0.0009890084450211564\n",
      "Loss -- 353.74139404296875   Current Learning Rate 0.000988856638266232\n",
      "Loss -- 342.01641845703125   Current Learning Rate 0.0009887048548127162\n",
      "Loss -- 390.1827697753906   Current Learning Rate 0.0009885530946570334\n",
      "Loss -- 369.8646545410156   Current Learning Rate 0.0009884013577956066\n",
      "Loss -- 348.78631591796875   Current Learning Rate 0.0009882496442248623\n",
      "Loss -- 378.8287048339844   Current Learning Rate 0.0009880979539412234\n",
      "Loss -- 360.9741516113281   Current Learning Rate 0.0009879462869411175\n",
      "---------\n",
      "At epoch 8\n",
      "Loss -- 351.2834777832031   Current Learning Rate 0.0009877946432209695\n",
      "Loss -- 354.1477355957031   Current Learning Rate 0.0009876430227772068\n",
      "Loss -- 388.5772399902344   Current Learning Rate 0.0009874914256062556\n",
      "Loss -- 371.6223449707031   Current Learning Rate 0.000987339851704543\n",
      "Loss -- 364.6509704589844   Current Learning Rate 0.0009871883010685\n",
      "Loss -- 333.8236083984375   Current Learning Rate 0.0009870367736945535\n",
      "Loss -- 347.8392639160156   Current Learning Rate 0.0009868852695791316\n",
      "Loss -- 374.257080078125   Current Learning Rate 0.0009867337887186677\n",
      "Loss -- 347.3980407714844   Current Learning Rate 0.00098658233110959\n",
      "Loss -- 330.4946594238281   Current Learning Rate 0.0009864308967483307\n",
      "---------\n",
      "At epoch 9\n",
      "Loss -- 319.3807373046875   Current Learning Rate 0.0009862794856313212\n",
      "Loss -- 327.6464538574219   Current Learning Rate 0.000986128097754992\n",
      "Loss -- 320.8393859863281   Current Learning Rate 0.0009859767331157785\n",
      "Loss -- 357.6723327636719   Current Learning Rate 0.0009858253917101112\n",
      "Loss -- 357.4667053222656   Current Learning Rate 0.0009856740735344263\n",
      "Loss -- 363.7883605957031   Current Learning Rate 0.0009855227785851572\n",
      "Loss -- 346.7189025878906   Current Learning Rate 0.000985371506858738\n",
      "Loss -- 357.47625732421875   Current Learning Rate 0.0009852202583516057\n",
      "Loss -- 358.8086242675781   Current Learning Rate 0.0009850690330601946\n",
      "Loss -- 357.26055908203125   Current Learning Rate 0.000984917830980942\n",
      "---------\n",
      "At epoch 10\n",
      "Loss -- 326.16473388671875   Current Learning Rate 0.0009847666521102842\n",
      "Loss -- 358.19476318359375   Current Learning Rate 0.0009846154964446596\n",
      "Loss -- 335.2423400878906   Current Learning Rate 0.0009844643639805072\n",
      "Loss -- 330.15362548828125   Current Learning Rate 0.0009843132547142654\n",
      "Loss -- 295.32781982421875   Current Learning Rate 0.0009841621686423718\n",
      "Loss -- 323.6325378417969   Current Learning Rate 0.0009840111057612681\n",
      "Loss -- 336.2218017578125   Current Learning Rate 0.0009838600660673936\n",
      "Loss -- 315.695068359375   Current Learning Rate 0.0009837090495571886\n",
      "Loss -- 335.85711669921875   Current Learning Rate 0.0009835580562270957\n",
      "Loss -- 327.5064697265625   Current Learning Rate 0.0009834070860735566\n",
      "---------\n",
      "At epoch 11\n",
      "Loss -- 353.2628173828125   Current Learning Rate 0.000983256139093015\n",
      "Loss -- 342.1965637207031   Current Learning Rate 0.0009831052152819123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss -- 305.82196044921875   Current Learning Rate 0.0009829543146366926\n",
      "Loss -- 353.1739807128906   Current Learning Rate 0.0009828034371538003\n",
      "Loss -- 311.3593444824219   Current Learning Rate 0.0009826525828296807\n",
      "Loss -- 340.68359375   Current Learning Rate 0.000982501751660778\n",
      "Loss -- 355.1368103027344   Current Learning Rate 0.0009823509436435378\n",
      "Loss -- 346.6053161621094   Current Learning Rate 0.0009822001587744065\n",
      "Loss -- 347.104248046875   Current Learning Rate 0.000982049397049831\n",
      "Loss -- 291.09332275390625   Current Learning Rate 0.0009818986584662606\n",
      "---------\n",
      "At epoch 12\n",
      "Loss -- 321.71795654296875   Current Learning Rate 0.000981747943020142\n",
      "Loss -- 317.8678894042969   Current Learning Rate 0.000981597250707923\n",
      "Loss -- 313.17852783203125   Current Learning Rate 0.0009814465815260534\n",
      "Loss -- 303.6146545410156   Current Learning Rate 0.0009812959354709842\n",
      "Loss -- 329.03765869140625   Current Learning Rate 0.000981145312539163\n",
      "Loss -- 308.5510559082031   Current Learning Rate 0.0009809947127270425\n",
      "Loss -- 322.8030700683594   Current Learning Rate 0.0009808441360310725\n",
      "Loss -- 309.4477233886719   Current Learning Rate 0.0009806935824477046\n",
      "Loss -- 324.5470275878906   Current Learning Rate 0.0009805430519733928\n",
      "Loss -- 309.5265808105469   Current Learning Rate 0.0009803925446045907\n",
      "---------\n",
      "At epoch 13\n",
      "Loss -- 311.7008056640625   Current Learning Rate 0.0009802420603377494\n",
      "Loss -- 314.7254638671875   Current Learning Rate 0.0009800915991693246\n",
      "Loss -- 277.9281005859375   Current Learning Rate 0.0009799411610957687\n",
      "Loss -- 328.3497619628906   Current Learning Rate 0.000979790746113539\n",
      "Loss -- 316.16827392578125   Current Learning Rate 0.0009796403542190893\n",
      "Loss -- 299.1598205566406   Current Learning Rate 0.000979489985408878\n",
      "Loss -- 317.85772705078125   Current Learning Rate 0.0009793396396793593\n",
      "Loss -- 315.158447265625   Current Learning Rate 0.0009791893170269933\n",
      "Loss -- 301.4400634765625   Current Learning Rate 0.0009790390174482342\n",
      "Loss -- 325.44696044921875   Current Learning Rate 0.000978888740939544\n",
      "---------\n",
      "At epoch 14\n",
      "Loss -- 328.188232421875   Current Learning Rate 0.0009787384874973798\n",
      "Loss -- 344.5408935546875   Current Learning Rate 0.0009785882571182005\n",
      "Loss -- 327.1204833984375   Current Learning Rate 0.0009784380497984654\n",
      "Loss -- 305.18975830078125   Current Learning Rate 0.0009782878655346379\n",
      "Loss -- 324.6885070800781   Current Learning Rate 0.0009781377043231773\n",
      "Loss -- 332.3841552734375   Current Learning Rate 0.0009779875661605451\n",
      "Loss -- 278.4691162109375   Current Learning Rate 0.000977837451043203\n",
      "Loss -- 325.8599853515625   Current Learning Rate 0.0009776873589676146\n",
      "Loss -- 294.3446960449219   Current Learning Rate 0.0009775372899302431\n",
      "Loss -- 299.7947082519531   Current Learning Rate 0.000977387243927553\n",
      "---------\n",
      "At epoch 15\n",
      "Loss -- 309.0517883300781   Current Learning Rate 0.0009772372209560071\n",
      "Loss -- 330.7562255859375   Current Learning Rate 0.0009770872210120708\n",
      "Loss -- 317.4724426269531   Current Learning Rate 0.0009769372440922106\n",
      "Loss -- 312.2016296386719   Current Learning Rate 0.0009767872901928906\n",
      "Loss -- 297.12310791015625   Current Learning Rate 0.0009766373593105783\n",
      "Loss -- 287.7879638671875   Current Learning Rate 0.0009764874514417409\n",
      "Loss -- 278.5741882324219   Current Learning Rate 0.0009763375665828457\n",
      "Loss -- 275.120849609375   Current Learning Rate 0.0009761877047303608\n",
      "Loss -- 304.5024719238281   Current Learning Rate 0.0009760378658807547\n",
      "Loss -- 281.3522644042969   Current Learning Rate 0.000975888050030497\n",
      "---------\n",
      "At epoch 16\n",
      "Loss -- 284.7207336425781   Current Learning Rate 0.0009757382571760568\n",
      "Loss -- 284.63983154296875   Current Learning Rate 0.000975588487313905\n",
      "Loss -- 280.610107421875   Current Learning Rate 0.0009754387404405122\n",
      "Loss -- 286.1786193847656   Current Learning Rate 0.0009752890165523494\n",
      "Loss -- 287.89923095703125   Current Learning Rate 0.0009751393156458893\n",
      "Loss -- 298.1129150390625   Current Learning Rate 0.0009749896377176037\n",
      "Loss -- 317.44891357421875   Current Learning Rate 0.0009748399827639651\n",
      "Loss -- 273.1072998046875   Current Learning Rate 0.0009746903507814482\n",
      "Loss -- 286.3875732421875   Current Learning Rate 0.0009745407417665268\n",
      "Loss -- 285.7281188964844   Current Learning Rate 0.000974391155715675\n",
      "---------\n",
      "At epoch 17\n",
      "Loss -- 304.116455078125   Current Learning Rate 0.0009742415926253687\n",
      "Loss -- 274.1561279296875   Current Learning Rate 0.000974092052492083\n",
      "Loss -- 287.5242614746094   Current Learning Rate 0.0009739425353122937\n",
      "Loss -- 285.41900634765625   Current Learning Rate 0.0009737930410824783\n",
      "Loss -- 300.3075256347656   Current Learning Rate 0.0009736435697991144\n",
      "Loss -- 304.7742614746094   Current Learning Rate 0.0009734941214586793\n",
      "Loss -- 284.2266540527344   Current Learning Rate 0.000973344696057651\n",
      "Loss -- 274.2557678222656   Current Learning Rate 0.0009731952935925098\n",
      "Loss -- 307.319580078125   Current Learning Rate 0.0009730459140597343\n",
      "Loss -- 282.56817626953125   Current Learning Rate 0.0009728965574558046\n",
      "---------\n",
      "At epoch 18\n",
      "Loss -- 287.3365783691406   Current Learning Rate 0.0009727472237772013\n",
      "Loss -- 299.7187194824219   Current Learning Rate 0.000972597913020405\n",
      "Loss -- 252.8444366455078   Current Learning Rate 0.0009724486251818979\n",
      "Loss -- 286.8872985839844   Current Learning Rate 0.0009722993602581625\n",
      "Loss -- 299.887939453125   Current Learning Rate 0.0009721501182456805\n",
      "Loss -- 292.5471496582031   Current Learning Rate 0.0009720008991409355\n",
      "Loss -- 263.7131042480469   Current Learning Rate 0.0009718517029404117\n",
      "Loss -- 301.2408142089844   Current Learning Rate 0.0009717025296405936\n",
      "Loss -- 283.6034851074219   Current Learning Rate 0.0009715533792379661\n",
      "Loss -- 281.271484375   Current Learning Rate 0.0009714042517290139\n",
      "---------\n",
      "At epoch 19\n",
      "Loss -- 304.3437194824219   Current Learning Rate 0.0009712551471102236\n",
      "Loss -- 287.2364501953125   Current Learning Rate 0.0009711060653780818\n",
      "Loss -- 286.74334716796875   Current Learning Rate 0.000970957006529075\n",
      "Loss -- 266.7640075683594   Current Learning Rate 0.0009708079705596908\n",
      "Loss -- 308.7857360839844   Current Learning Rate 0.0009706589574664174\n",
      "Loss -- 271.7149658203125   Current Learning Rate 0.000970509967245744\n",
      "Loss -- 288.3763732910156   Current Learning Rate 0.0009703609998941592\n",
      "Loss -- 292.1898193359375   Current Learning Rate 0.0009702120554081528\n",
      "Loss -- 290.067626953125   Current Learning Rate 0.0009700631337842146\n",
      "Loss -- 314.29876708984375   Current Learning Rate 0.000969914235018837\n",
      "---------\n",
      "At epoch 20\n",
      "Loss -- 276.2752685546875   Current Learning Rate 0.0009697653591085103\n",
      "Loss -- 272.46527099609375   Current Learning Rate 0.0009696165060497262\n",
      "Loss -- 296.0527038574219   Current Learning Rate 0.0009694676758389776\n",
      "Loss -- 296.17218017578125   Current Learning Rate 0.0009693188684727566\n",
      "Loss -- 277.62530517578125   Current Learning Rate 0.0009691700839475577\n",
      "Loss -- 285.345458984375   Current Learning Rate 0.0009690213222598745\n",
      "Loss -- 278.9995422363281   Current Learning Rate 0.0009688725834062016\n",
      "Loss -- 290.95611572265625   Current Learning Rate 0.0009687238673830343\n",
      "Loss -- 278.5080261230469   Current Learning Rate 0.0009685751741868679\n",
      "Loss -- 285.3985900878906   Current Learning Rate 0.000968426503814199\n",
      "---------\n",
      "At epoch 21\n",
      "Loss -- 275.0262756347656   Current Learning Rate 0.0009682778562615241\n",
      "Loss -- 298.46551513671875   Current Learning Rate 0.0009681292315253404\n",
      "Loss -- 263.6994323730469   Current Learning Rate 0.0009679806296021457\n",
      "Loss -- 270.8441162109375   Current Learning Rate 0.0009678320504884393\n",
      "Loss -- 292.8564758300781   Current Learning Rate 0.0009676834941807183\n",
      "Loss -- 272.54510498046875   Current Learning Rate 0.0009675349606754829\n",
      "Loss -- 272.24249267578125   Current Learning Rate 0.0009673864499692338\n",
      "Loss -- 257.4828186035156   Current Learning Rate 0.0009672379620584711\n",
      "Loss -- 232.64230346679688   Current Learning Rate 0.0009670894969396956\n",
      "Loss -- 254.05345153808594   Current Learning Rate 0.0009669410546094084\n",
      "---------\n",
      "At epoch 22\n",
      "Loss -- 240.1870574951172   Current Learning Rate 0.0009667926350641129\n",
      "Loss -- 279.6591491699219   Current Learning Rate 0.0009666442383003103\n",
      "Loss -- 261.4538879394531   Current Learning Rate 0.0009664958643145045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss -- 282.05621337890625   Current Learning Rate 0.0009663475131031995\n",
      "Loss -- 244.62484741210938   Current Learning Rate 0.0009661991846628994\n",
      "Loss -- 302.17266845703125   Current Learning Rate 0.0009660508789901079\n",
      "Loss -- 272.8968811035156   Current Learning Rate 0.0009659025960813318\n",
      "Loss -- 277.6354064941406   Current Learning Rate 0.0009657543359330767\n",
      "Loss -- 279.40777587890625   Current Learning Rate 0.000965606098541848\n",
      "Loss -- 261.5422668457031   Current Learning Rate 0.0009654578839041538\n",
      "---------\n",
      "At epoch 23\n",
      "Loss -- 245.94265747070312   Current Learning Rate 0.0009653096920165011\n",
      "Loss -- 292.3544006347656   Current Learning Rate 0.000965161522875398\n",
      "Loss -- 300.3241882324219   Current Learning Rate 0.0009650133764773521\n",
      "Loss -- 265.1915588378906   Current Learning Rate 0.0009648652528188737\n",
      "Loss -- 257.6287536621094   Current Learning Rate 0.0009647171518964724\n",
      "Loss -- 256.72235107421875   Current Learning Rate 0.0009645690737066583\n",
      "Loss -- 256.3341369628906   Current Learning Rate 0.0009644210182459419\n",
      "Loss -- 260.3723449707031   Current Learning Rate 0.0009642729855108344\n",
      "Loss -- 273.2748107910156   Current Learning Rate 0.0009641249754978468\n",
      "Loss -- 271.71466064453125   Current Learning Rate 0.0009639769882034921\n",
      "---------\n",
      "At epoch 24\n",
      "Loss -- 279.2529296875   Current Learning Rate 0.0009638290236242827\n",
      "Loss -- 263.6417541503906   Current Learning Rate 0.000963681081756733\n",
      "Loss -- 278.2190856933594   Current Learning Rate 0.0009635331625973562\n",
      "Loss -- 252.1101531982422   Current Learning Rate 0.0009633852661426668\n",
      "Loss -- 291.41973876953125   Current Learning Rate 0.0009632373923891802\n",
      "Loss -- 250.59019470214844   Current Learning Rate 0.0009630895413334108\n",
      "Loss -- 269.35906982421875   Current Learning Rate 0.000962941712971875\n",
      "Loss -- 269.8728942871094   Current Learning Rate 0.0009627939073010904\n",
      "Loss -- 263.0137023925781   Current Learning Rate 0.0009626461243175733\n",
      "Loss -- 269.0028991699219   Current Learning Rate 0.0009624983640178417\n",
      "---------\n",
      "At epoch 25\n",
      "Loss -- 302.0216369628906   Current Learning Rate 0.0009623506263984136\n",
      "Loss -- 255.22787475585938   Current Learning Rate 0.0009622029114558075\n",
      "Loss -- 268.3298034667969   Current Learning Rate 0.0009620552191865426\n",
      "Loss -- 291.9926452636719   Current Learning Rate 0.0009619075495871388\n",
      "Loss -- 273.0228271484375   Current Learning Rate 0.0009617599026541167\n",
      "Loss -- 268.56988525390625   Current Learning Rate 0.0009616122783839967\n",
      "Loss -- 252.70555114746094   Current Learning Rate 0.0009614646767732998\n",
      "Loss -- 238.3632049560547   Current Learning Rate 0.000961317097818549\n",
      "Loss -- 258.3166809082031   Current Learning Rate 0.0009611695415162659\n",
      "Loss -- 252.64871215820312   Current Learning Rate 0.0009610220078629743\n",
      "---------\n",
      "At epoch 26\n",
      "Loss -- 257.25518798828125   Current Learning Rate 0.0009608744968551973\n",
      "Loss -- 270.4351806640625   Current Learning Rate 0.000960727008489459\n",
      "Loss -- 266.67633056640625   Current Learning Rate 0.0009605795427622838\n",
      "Loss -- 313.3527526855469   Current Learning Rate 0.0009604320996701962\n",
      "Loss -- 260.4281311035156   Current Learning Rate 0.0009602846792097231\n",
      "Loss -- 289.51165771484375   Current Learning Rate 0.0009601372813773903\n",
      "Loss -- 266.03338623046875   Current Learning Rate 0.0009599899061697239\n",
      "Loss -- 307.81494140625   Current Learning Rate 0.0009598425535832518\n",
      "Loss -- 273.95245361328125   Current Learning Rate 0.0009596952236145018\n",
      "Loss -- 244.79539489746094   Current Learning Rate 0.0009595479162600026\n",
      "---------\n",
      "At epoch 27\n",
      "Loss -- 276.95355224609375   Current Learning Rate 0.0009594006315162814\n",
      "Loss -- 252.24667358398438   Current Learning Rate 0.0009592533693798694\n",
      "Loss -- 305.5506591796875   Current Learning Rate 0.0009591061298472962\n",
      "Loss -- 273.23553466796875   Current Learning Rate 0.0009589589129150915\n",
      "Loss -- 234.40357971191406   Current Learning Rate 0.0009588117185797862\n",
      "Loss -- 283.841064453125   Current Learning Rate 0.0009586645468379124\n",
      "Loss -- 249.9264678955078   Current Learning Rate 0.0009585173976860023\n",
      "Loss -- 249.0546417236328   Current Learning Rate 0.0009583702711205875\n",
      "Loss -- 272.8722229003906   Current Learning Rate 0.0009582231671382017\n",
      "Loss -- 280.5869140625   Current Learning Rate 0.0009580760857353792\n",
      "---------\n",
      "At epoch 28\n",
      "Loss -- 298.4685974121094   Current Learning Rate 0.0009579290269086534\n",
      "Loss -- 237.98086547851562   Current Learning Rate 0.0009577819906545597\n",
      "Loss -- 239.8138885498047   Current Learning Rate 0.0009576349769696321\n",
      "Loss -- 265.5005798339844   Current Learning Rate 0.0009574879858504078\n",
      "Loss -- 251.11172485351562   Current Learning Rate 0.0009573410172934221\n",
      "Loss -- 274.9896545410156   Current Learning Rate 0.0009571940712952121\n",
      "Loss -- 248.18963623046875   Current Learning Rate 0.0009570471478523151\n",
      "Loss -- 268.1526184082031   Current Learning Rate 0.0009569002469612691\n",
      "Loss -- 246.1899871826172   Current Learning Rate 0.0009567533686186128\n",
      "Loss -- 264.1610412597656   Current Learning Rate 0.0009566065128208851\n",
      "---------\n",
      "At epoch 29\n",
      "Loss -- 271.6862487792969   Current Learning Rate 0.0009564596795646251\n",
      "Loss -- 274.73028564453125   Current Learning Rate 0.0009563128688463728\n",
      "Loss -- 240.8125457763672   Current Learning Rate 0.0009561660806626692\n",
      "Loss -- 266.8802490234375   Current Learning Rate 0.000956019315010055\n",
      "Loss -- 256.32562255859375   Current Learning Rate 0.0009558725718850715\n",
      "Loss -- 242.94810485839844   Current Learning Rate 0.0009557258512842621\n",
      "Loss -- 279.7922668457031   Current Learning Rate 0.0009555791532041676\n",
      "Loss -- 257.4573059082031   Current Learning Rate 0.0009554324776413324\n",
      "Loss -- 256.6026306152344   Current Learning Rate 0.0009552858245923007\n",
      "Loss -- 262.4327392578125   Current Learning Rate 0.0009551391940536155\n",
      "---------\n",
      "At epoch 30\n",
      "Loss -- 244.52874755859375   Current Learning Rate 0.000954992586021823\n",
      "Loss -- 238.1540985107422   Current Learning Rate 0.0009548460004934674\n",
      "Loss -- 266.13360595703125   Current Learning Rate 0.0009546994374650951\n",
      "Loss -- 266.2850646972656   Current Learning Rate 0.000954552896933253\n",
      "Loss -- 229.37686157226562   Current Learning Rate 0.0009544063788944871\n",
      "Loss -- 272.44781494140625   Current Learning Rate 0.0009542598833453453\n",
      "Loss -- 237.43190002441406   Current Learning Rate 0.000954113410282375\n",
      "Loss -- 258.849853515625   Current Learning Rate 0.0009539669597021251\n",
      "Loss -- 243.4106903076172   Current Learning Rate 0.0009538205316011449\n",
      "Loss -- 289.490966796875   Current Learning Rate 0.0009536741259759833\n",
      "---------\n",
      "At epoch 31\n",
      "Loss -- 269.12841796875   Current Learning Rate 0.0009535277428231911\n",
      "Loss -- 275.12432861328125   Current Learning Rate 0.0009533813821393186\n",
      "Loss -- 248.36671447753906   Current Learning Rate 0.0009532350439209173\n",
      "Loss -- 272.7108459472656   Current Learning Rate 0.0009530887281645386\n",
      "Loss -- 254.08486938476562   Current Learning Rate 0.0009529424348667347\n",
      "Loss -- 254.37066650390625   Current Learning Rate 0.0009527961640240581\n",
      "Loss -- 269.46746826171875   Current Learning Rate 0.0009526499156330622\n",
      "Loss -- 237.16880798339844   Current Learning Rate 0.0009525036896903012\n",
      "Loss -- 258.949951171875   Current Learning Rate 0.0009523574861923288\n",
      "Loss -- 252.00482177734375   Current Learning Rate 0.000952211305135701\n",
      "---------\n",
      "At epoch 32\n",
      "Loss -- 246.6485137939453   Current Learning Rate 0.000952065146516972\n",
      "Loss -- 252.9606475830078   Current Learning Rate 0.0009519190103326984\n",
      "Loss -- 251.80372619628906   Current Learning Rate 0.0009517728965794357\n",
      "Loss -- 248.09561157226562   Current Learning Rate 0.0009516268052537423\n",
      "Loss -- 301.196044921875   Current Learning Rate 0.0009514807363521751\n",
      "Loss -- 268.0025634765625   Current Learning Rate 0.0009513346898712919\n",
      "Loss -- 244.0081024169922   Current Learning Rate 0.0009511886658076508\n",
      "Loss -- 244.83509826660156   Current Learning Rate 0.0009510426641578124\n",
      "Loss -- 250.75341796875   Current Learning Rate 0.0009508966849183348\n",
      "Loss -- 247.72630310058594   Current Learning Rate 0.0009507507280857792\n",
      "---------\n",
      "At epoch 33\n",
      "Loss -- 247.59259033203125   Current Learning Rate 0.0009506047936567056\n",
      "Loss -- 279.6233215332031   Current Learning Rate 0.0009504588816276749\n",
      "Loss -- 259.6806945800781   Current Learning Rate 0.0009503129919952494\n",
      "Loss -- 277.71380615234375   Current Learning Rate 0.0009501671247559918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss -- 237.69117736816406   Current Learning Rate 0.0009500212799064642\n",
      "Loss -- 252.71580505371094   Current Learning Rate 0.0009498754574432299\n",
      "Loss -- 210.03765869140625   Current Learning Rate 0.0009497296573628529\n",
      "Loss -- 252.3270263671875   Current Learning Rate 0.0009495838796618983\n",
      "Loss -- 254.95640563964844   Current Learning Rate 0.0009494381243369304\n",
      "Loss -- 279.1491394042969   Current Learning Rate 0.0009492923913845141\n",
      "---------\n",
      "At epoch 34\n",
      "Loss -- 261.0584411621094   Current Learning Rate 0.0009491466808012157\n",
      "Loss -- 265.0417785644531   Current Learning Rate 0.0009490009925836022\n",
      "Loss -- 250.34921264648438   Current Learning Rate 0.000948855326728239\n",
      "Loss -- 252.2300262451172   Current Learning Rate 0.0009487096832316957\n",
      "Loss -- 239.07928466796875   Current Learning Rate 0.0009485640620905388\n",
      "Loss -- 256.37420654296875   Current Learning Rate 0.0009484184633013377\n",
      "Loss -- 241.4615020751953   Current Learning Rate 0.0009482728868606611\n",
      "Loss -- 235.75682067871094   Current Learning Rate 0.0009481273327650792\n",
      "Loss -- 264.8072509765625   Current Learning Rate 0.0009479818010111616\n",
      "Loss -- 246.8888702392578   Current Learning Rate 0.0009478362915954799\n",
      "---------\n",
      "At epoch 35\n",
      "Loss -- 252.1347198486328   Current Learning Rate 0.0009476908045146047\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-24756538c93c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mglobal_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         output_gradients=output_gradients)\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       output_gradients)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m def _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs,\n\u001b[0m\u001b[1;32m     98\u001b[0m                        out_grads):\n\u001b[1;32m     99\u001b[0m   \"\"\"Calls the gradient function of the op.\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(300):\n",
    "    print(f\"At epoch {e}\")\n",
    "    for i, img in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(vae.variables)\n",
    "\n",
    "            latent, out, mean, log_var, kl_map = vae(img)\n",
    "            flat_mean = tf.reshape(mean, (-1, latent_size))\n",
    "            flat_log_var = tf.reshape(log_var, (-1, latent_size))\n",
    "\n",
    "            cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=out, labels=img)\n",
    "            logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "            kl_loss = 0.5 * tf.reduce_sum(tf.exp(flat_log_var) + flat_mean**2 - 1. - flat_log_var, \n",
    "                                          axis=[1])\n",
    "            kl_loss = tf.reduce_sum(tf.reshape(kl_loss, (-1, object_number)), axis=1)\n",
    "            loss = -tf.reduce_mean(logpx_z - kl_loss)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Loss -- {loss}   Current Learning Rate {learning_rate}\")\n",
    "                with summary_writer.as_default(), tf.contrib.summary.always_record_summaries():\n",
    "                    tf.contrib.summary.scalar('Loss', loss)\n",
    "                    tf.contrib.summary.image('Before', img)\n",
    "                    tf.contrib.summary.image('Sample', tf.nn.sigmoid(out))\n",
    "                    \n",
    "        \n",
    "        grad = tape.gradient(loss, vae.variables)\n",
    "        global_step.assign_add(1)\n",
    "        optimizer.apply_gradients(zip(grad, vae.variables))\n",
    "        \n",
    "    print(f\"---------\")\n",
    "    saver.save(f\"save/{experiment_name}/{experiment_name}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182bc186d8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images_grid(tf.squeeze(img).numpy(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a30f65b38>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images_grid(tf.squeeze(tf.nn.sigmoid(out)).numpy(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl_attention_map(kl_index):\n",
    "    batch_size = kl_index.shape[0]\n",
    "    blank_map = np.zeros((batch_size, 14*14))\n",
    "    for i in range(batch_size):\n",
    "        blank_map[i] = kl_index[i]\n",
    "    return blank_map.reshape(batch_size, 14, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a31d69780>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images_grid(get_kl_attention_map(kl_map), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
